{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59aa2ebd",
   "metadata": {},
   "source": [
    "# Assignment No - 5\n",
    "\n",
    "## Implementing Multiple Machine Learning Algorithms\n",
    "\n",
    "In this notebook, we will implement various Machine Learning algorithms on a dataset. The algorithms covered are:\n",
    "\n",
    "- Simple Linear Regression\n",
    "- Multiple Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Tree Regressor\n",
    "- Decision Tree Classifier\n",
    "- Random Forest\n",
    "- Naive Bayes\n",
    "- Support Vector Classifier (SVC)\n",
    "- XGBoost\n",
    "\n",
    "### Step 1: Load the Dataset and Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0946a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Load Sample Dataset (Iris for Classification, Boston Housing for Regression)\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "\n",
    "# Load classification dataset (Iris)\n",
    "iris = load_iris()\n",
    "X_class, y_class = iris.data, iris.target\n",
    "\n",
    "# Load regression dataset (Diabetes)\n",
    "diabetes = load_diabetes()\n",
    "X_reg, y_reg = diabetes.data, diabetes.target\n",
    "\n",
    "# Split datasets into training and testing sets\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_reg = scaler.fit_transform(X_train_reg)\n",
    "X_test_reg = scaler.transform(X_test_reg)\n",
    "X_train_class = scaler.fit_transform(X_train_class)\n",
    "X_test_class = scaler.transform(X_test_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c058b",
   "metadata": {},
   "source": [
    "## Step 2: Implement ML Algorithms\n",
    "\n",
    "### Simple Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90621fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_reg, y_train_reg)\n",
    "y_pred_lr = lr.predict(X_test_reg)\n",
    "print(\"MSE for Linear Regression:\", mean_squared_error(y_test_reg, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression (Same as Simple LR since it considers multiple features)\n",
    "# Already implemented above\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train_class, y_train_class)\n",
    "y_pred_log = log_reg.predict(X_test_class)\n",
    "print(\"Accuracy for Logistic Regression:\", accuracy_score(y_test_class, y_pred_log))\n",
    "\n",
    "# Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "dt_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_dt_reg = dt_reg.predict(X_test_reg)\n",
    "print(\"MSE for Decision Tree Regressor:\", mean_squared_error(y_test_reg, y_pred_dt_reg))\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_dt_clf = dt_clf.predict(X_test_class)\n",
    "print(\"Accuracy for Decision Tree Classifier:\", accuracy_score(y_test_class, y_pred_dt_clf))\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_class, y_train_class)\n",
    "y_pred_rf = rf.predict(X_test_class)\n",
    "print(\"Accuracy for Random Forest:\", accuracy_score(y_test_class, y_pred_rf))\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_class, y_train_class)\n",
    "y_pred_nb = nb.predict(X_test_class)\n",
    "print(\"Accuracy for Naive Bayes:\", accuracy_score(y_test_class, y_pred_nb))\n",
    "\n",
    "# Support Vector Classifier (SVC)\n",
    "svc = SVC()\n",
    "svc.fit(X_train_class, y_train_class)\n",
    "y_pred_svc = svc.predict(X_test_class)\n",
    "print(\"Accuracy for SVC:\", accuracy_score(y_test_class, y_pred_svc))\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_class)\n",
    "print(\"Accuracy for XGBoost:\", accuracy_score(y_test_class, y_pred_xgb))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
